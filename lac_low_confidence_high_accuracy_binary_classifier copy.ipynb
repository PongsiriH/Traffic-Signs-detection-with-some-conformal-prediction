{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_confidence_classifier(label):\n",
    "    \"\"\"\n",
    "    A simple binary classifier that take in labels\n",
    "    and returns correct low confidence scores.\n",
    "    \n",
    "    :param label: The input label (0 or 1).\n",
    "    :return: A tuple of (label, confidence_score), where confidence_score is close to 0.5.\n",
    "    \"\"\"\n",
    "    num_inputs = len(label)\n",
    "    estimated_probabilities = np.zeros((num_inputs, 2))\n",
    "    # confidences close to 0.5\n",
    "    estimated_probabilities[range(num_inputs), label] = 0.55 + np.random.uniform(-0.04, 0.05, num_inputs)\n",
    "    return estimated_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier will have 100% accuracy with low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53267528, 0.        ],\n",
       "       [0.        , 0.57758511],\n",
       "       [0.        , 0.59655905],\n",
       "       [0.53499038, 0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_labels = [0, 1, 1, 0]\n",
    "prediction0 = low_confidence_classifier(example_labels)\n",
    "prediction0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let set $\\alpha=0.05$ expcting coverage = 0.95. We will ignore the finite-sampling correction in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let generate some targets and measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_targets = [0]*100 + [1]*100\n",
    "\n",
    "cal_pred = low_confidence_classifier(cal_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515652592778679"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonconformity_scores = cal_pred[range(len(cal_targets)), cal_targets] # confidence on correct label\n",
    "qhat = np.quantile(nonconformity_scores, q=ALPHA)\n",
    "cal_conformal_pred = cal_pred > qhat # inference\n",
    "qhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate on calibration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "correct_prediction = np.equal(cal_pred.argmax(axis=1), cal_targets)\n",
    "accuracy = correct_prediction.sum() / len(cal_targets)\n",
    "\n",
    "# coverage\n",
    "coverage = cal_conformal_pred[range(len(cal_targets)), cal_targets]\n",
    "coverage = coverage.sum() / len(cal_targets)\n",
    "\n",
    "print(accuracy)\n",
    "print(coverage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
