{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run in colab: https://colab.research.google.com/drive/1HSqgNwxLLsNKpAoIMJp1FfHbN-WIW-wU#scrollTo=0xUaMXqXPwlT\n",
    "\n",
    "only tested on T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cloning yolov5 reporsitory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clone yolov5 repository with specific version.\n",
    "git clone https://github.com/ultralytics/yolov5\n",
    "cd yolov5\n",
    "git reset --hard 9cdbd1de6b64193b444365982427d5f6d48d6a97\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, subprocess\n",
    "sys.path.append('yolov5')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import yaml\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.general import check_img_size, xywh2xyxy\n",
    "from yolov5.utils.dataloaders import LoadImages, LoadScreenshots, LoadStreams, LoadImagesAndLabels\n",
    "\n",
    "import holoviews as hv\n",
    "%env HV_DOC_HTML=true\n",
    "hv.extension('bokeh')\n",
    "\n",
    "from traffic_signs_detection_conformal.general import (process_label, preprocess_image, one_hot_encode, apply_conf_thresh, nms_resize_organize_etc)\n",
    "from traffic_signs_detection_conformal.general import iou as box_iou\n",
    "from traffic_signs_detection_conformal.general import YOLO2SKLEARN # sklearn-like interface for yolo. just my preference.\n",
    "from traffic_signs_detection_conformal.general import adjust_bbox_for_flipud, label_bbox # for plotting.\n",
    "from traffic_signs_detection_conformal.general import megdown\n",
    "\n",
    "from traffic_signs_detection_conformal.core import gather_nonconformity_scores, form_prediction_set, matching_iou_hungarian\n",
    "from traffic_signs_detection_conformal.core_utils import predictions_and_labels, summary_table_and_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'thah'  # 'thah' (will add 'gtsdb' at some point.)\n",
    "if dataset_name == 'thah':\n",
    "  megdown('yolov5.pt', '1sdJcel3M_Y9rkCdk9G8GfYaerln8ciJu', dataset_name) # yolov5 model\n",
    "  megdown('thah.zip', '1JKmMFoKq5U1wo4_86CfvAuM2N1lLpq55', dataset_name) # thai dataset\n",
    "  !unzip 'datasets/thah/thah.zip' -d 'datasets/thah/cal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for pytorch\n",
    "DEVICE = 'cuda'\n",
    "DNN = True\n",
    "DATA = None\n",
    "half = True\n",
    "DEVICE = torch.device(DEVICE)\n",
    "\n",
    "# config for dataset\n",
    "WEIGHTS = \"datasets/thah/yolov5.pt\"\n",
    "path_dataset = 'datasets/thah/cal'\n",
    "path_cal = os.path.join(path_dataset, \"images\")\n",
    "num_classes = 36\n",
    "\n",
    "# config for yolo\n",
    "IMGSZ = 640\n",
    "IOU_THRESH = 0.7\n",
    "IOU_NMS_THRESH = 0.05\n",
    "CONF_THRESH = 0.5\n",
    "\n",
    "# config for conformal prediction\n",
    "ALPHA_COVERAGE =  0.10\n",
    "CP_METHOD = 'raps' # lac, aps, raps\n",
    "k_reg = 5\n",
    "lambda_reg = 0.01\n",
    "if CP_METHOD == 'lac': ALPHA_COVERAGE = 1 - ALPHA_COVERAGE\n",
    "\n",
    "# config for plotting\n",
    "class_mapping = None # if 'None', show number label.\n",
    "CAL_SIZE = 100\n",
    "\n",
    "if 'gtsdb'== dataset_name:\n",
    "    with open(\"D:/Year 3/A deep learning approach for Thai traffic sign recognition/train_ultralytics/dataset_settings/GTSDB_splitted_training_set_valsize300.yaml\", \"r\") as f:\n",
    "        dataset_ = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for key, name in dataset_['names'].items():\n",
    "        name: str\n",
    "        dataset_['names'][key] = (name\n",
    "                                .replace(\" (prohibitory)\", \"\")\n",
    "                                .replace(\" (other)\", \"\")\n",
    "                                .replace(\" (danger)\", \"\")\n",
    "                                .replace(\" (trucks)\", \"\")\n",
    "                                .replace(\" (mandatory)\", \"\")\n",
    "                                .replace(\" (overtaking)\", \"\")\n",
    "                                .replace(\" (overtaking (trucks))\", \"\")\n",
    "                                )\n",
    "    class_mapping = dataset_['names']\n",
    "elif 'thah'== dataset_name:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model and dataset. Split dataset into calibration set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "yolo = DetectMultiBackend(weights=WEIGHTS, device=DEVICE, dnn=DNN, data=DATA, fp16=half).to(DEVICE)\n",
    "model = YOLO2SKLEARN(yolo)\n",
    "# load dataset\n",
    "imgsz = check_img_size(IMGSZ, s=yolo.stride)\n",
    "dataset = LoadImagesAndLabels(path_cal, img_size=imgsz)\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "cal_indices = indices[:CAL_SIZE]\n",
    "test_indices = indices[CAL_SIZE:]\n",
    "print(\"\\nCalibration set has {} images\".format(CAL_SIZE))\n",
    "print(\"Test set has {} images\".format(len(dataset) - CAL_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather nonconformity scores on calibration set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonconformity_scores = []\n",
    "for idx in cal_indices: # for each image-label pair.\n",
    "    img, label, path, shape = dataset.__getitem__(idx)\n",
    "\n",
    "    # process image-label into desired format.\n",
    "    label = process_label(label)\n",
    "    img = preprocess_image(img, (IMGSZ, IMGSZ))\n",
    "\n",
    "    # yolo: making prediction and post-process.\n",
    "    with torch.no_grad():\n",
    "        pred = model(img) # forwarding yolo model\n",
    "    pred = apply_conf_thresh(pred, CONF_THRESH)\n",
    "    pred = nms_resize_organize_etc(pred, IOU_NMS_THRESH, imgsz)\n",
    "\n",
    "    # box matching\n",
    "    iou_matrix = box_iou(label[\"xyxy\"], pred['xyxy'])\n",
    "    i, j = matching_iou_hungarian(iou_matrix, IOU_THRESH)\n",
    "    gts_class = label[\"class_label\"][i]\n",
    "    pred_dist = pred['class_dist'][j]\n",
    "\n",
    "    if gts_class.shape[0] == 0 or pred_dist.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    gts_dist = one_hot_encode(gts_class, num_classes=num_classes)\n",
    "    ncs = gather_nonconformity_scores(gts_dist, pred_dist, CP_METHOD, k_reg, lambda_reg)\n",
    "    nonconformity_scores.extend(ncs)\n",
    "nonconformity_scores = np.hstack(nonconformity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute and plot quantile of nonconformity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(nonconformity_scores)\n",
    "quantile = np.quantile(nonconformity_scores, np.ceil((n + 1) * (1 - ALPHA_COVERAGE)) / n, method=\"higher\")\n",
    "\n",
    "print(\"Gathered total of {} nonconformity scores\".format(n))\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv_nonconformity_scores = hv.Histogram(np.histogram(nonconformity_scores, bins=50)).opts(width=1000, title='nonconformity scores')\n",
    "hv_nonconformity_quantile = hv.VLine(quantile).opts(color='red')\n",
    "hv_overlay = hv_nonconformity_scores * hv_nonconformity_quantile\n",
    "hv_overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make conformalized prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions, test_labels = predictions_and_labels(model, dataset, CONF_THRESH, IOU_NMS_THRESH, quantile, CP_METHOD, imgsz, k_reg, lambda_reg, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate box-wise coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary_table, test_result = summary_table_and_results(test_labels, test_predictions, IOU_THRESH)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample bounding-boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary_table.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize output plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(dataset))\n",
    "img, labels, path, shapes = dataset.__getitem__(test_indices[idx])\n",
    "\n",
    "img = img.permute(1,2,0).to('cpu').numpy()\n",
    "\n",
    "true_bbox = adjust_bbox_for_flipud(test_labels[idx]['xyxy'] - 0.5, 0)\n",
    "pred_bbox = adjust_bbox_for_flipud(xywh2xyxy(test_predictions[idx]['xywh']) - 0.5, 0)\n",
    "\n",
    "true_class = test_labels[idx].get('class_label_mapped', test_labels[idx]['class_label'])\n",
    "pred_class = test_predictions[idx]['class_label']\n",
    "pred_class_set = [np.where(ps)[0] for ps in test_predictions[idx]['prediction_set_array']]\n",
    "\n",
    "print(\"impath: \", test_predictions[idx]['impath'])\n",
    "print(\"target: \", true_class)\n",
    "print(\"point prediction: \", pred_class)\n",
    "print(\"set prediction:: \", pred_class_set)\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv_img = hv.RGB(img)\n",
    "hv_true_bbox = label_bbox(hv.Rectangles(true_bbox), true_class.astype(int), 'lightgreen', class_mapping)\n",
    "hv_pred_bbox = label_bbox(hv.Rectangles(pred_bbox), pred_class.astype(int), 'lightgreen', class_mapping)\n",
    "hv_pred_set_bbox = label_bbox(hv.Rectangles(pred_bbox), pred_class_set, 'lightgreen', class_mapping)\n",
    "\n",
    "hv_true = (hv_img * hv_true_bbox).opts(width=450, height=600, title='Ground-truth')\n",
    "hv_point_pred = (hv_img * hv_pred_bbox).opts(width=450, height=600, title='YOLO\\'s base prediction')\n",
    "hv_set_pred = (hv_img * hv_pred_set_bbox).opts(width=450, height=600, title='Conformalized class prediction')\n",
    "\n",
    "overlay = hv_true + hv_point_pred + hv_set_pred\n",
    "overlay.cols(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fndetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
